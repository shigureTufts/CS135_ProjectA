{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0e5d0d-8e26-4278-a555-b4b5a30fd1e4",
   "metadata": {},
   "source": [
    "# Project A: Classifying Sentiment from Text Reviews\n",
    "In this project, our team will try to perfrom Bag of Word approch to implement sentiment clasificaion on product review on websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67146029-59b1-4d1c-95bf-067e34e15c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import tokenize_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6355a07-f3ac-440c-a9b0-c5a31c576204",
   "metadata": {},
   "source": [
    "## Step1: Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2939be4d-8fa7-41db-82bd-290a23ee40c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     website_name                                               text\n",
      "0          amazon  Oh and I forgot to also mention the weird colo...\n",
      "1          amazon                       THAT one didn't work either.\n",
      "2          amazon                                 Waste of 13 bucks.\n",
      "3          amazon  Product is useless, since it does not have eno...\n",
      "4          amazon  None of the three sizes they sent with the hea...\n",
      "...           ...                                                ...\n",
      "2395         yelp  The sweet potato fries were very good and seas...\n",
      "2396         yelp  I could eat their bruschetta all day it is dev...\n",
      "2397         yelp                               Ambience is perfect.\n",
      "2398         yelp  We ordered the duck rare and it was pink and t...\n",
      "2399         yelp       Service was good and the company was better!\n",
      "\n",
      "[2400 rows x 2 columns]\n",
      "    website_name                                               text\n",
      "0         amazon  It only recognizes the Phone as its storage de...\n",
      "1         amazon  Disappointing accessory from a good manufacturer.\n",
      "2         amazon  The one big drawback of the MP3 player is that...\n",
      "3         amazon  This particular model would not work with my M...\n",
      "4         amazon  If the two were seperated by a mere 5+ ft I st...\n",
      "..           ...                                                ...\n",
      "595         yelp                Everything was fresh and delicious!\n",
      "596         yelp          - Really, really good rice, all the time.\n",
      "597         yelp                              Pretty awesome place.\n",
      "598         yelp        The staff are great, the ambiance is great.\n",
      "599         yelp            The patio seating was very comfortable.\n",
      "\n",
      "[600 rows x 2 columns]\n",
      "      is_positive_sentiment\n",
      "0                         0\n",
      "1                         0\n",
      "2                         0\n",
      "3                         0\n",
      "4                         0\n",
      "...                     ...\n",
      "2395                      1\n",
      "2396                      1\n",
      "2397                      1\n",
      "2398                      1\n",
      "2399                      1\n",
      "\n",
      "[2400 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# import datasets\n",
    "data_dir = 'data_reviews'\n",
    "x_train_df = pd.read_csv(os.path.join(data_dir, 'x_train.csv'))\n",
    "x_test_df = pd.read_csv(os.path.join(data_dir, 'x_test.csv'))\n",
    "y_train_df = pd.read_csv(os.path.join(data_dir, 'y_train.csv'))\n",
    "\n",
    "# debug you\n",
    "# printing out dataset\n",
    "print(x_train_df)\n",
    "print(x_test_df)\n",
    "print(y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c360262b-ddea-4497-8bbe-2bb383b81a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first five rows and last five rows\n",
    "N, n_cols = x_train_df.shape\n",
    "# print(\"Shape of x_train_df: (%d, %d)\" % (N, n_cols))\n",
    "# print(\"Shape of y_train_df: %s\" % str(y_train_df.shape))\n",
    "\n",
    "# Print out the first five rows and last five rows\n",
    "tr_text_list = x_train_df['text'].values.tolist()\n",
    "\n",
    "test_text_list = x_test_df['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051963c-50f9-4740-95f6-1e883eae3294",
   "metadata": {},
   "source": [
    "## Step2: Cleaning Imported Data and Count Vacabulary Friquency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d8101f-36ee-4d72-afff-c3c53484683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning raw data\n",
    "tok_count_dict = dict()\n",
    "for line in tr_text_list:\n",
    "    tok_list = tokenize_text.tokenize_text(line)\n",
    "    for tok in tok_list:\n",
    "        if tok in tok_count_dict:\n",
    "            tok_count_dict[tok] += 1\n",
    "        else:\n",
    "            tok_count_dict[tok] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a255ec-9c5a-4e13-9de1-1dc9528d1277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1560 the\n",
      "  916 and\n",
      "  707 a\n",
      "  700 i\n",
      "  609 is\n",
      "  542 to\n",
      "  534 it\n",
      "  493 of\n",
      "  493 this\n",
      "  447 was\n",
      "  328 in\n",
      "  267 \n",
      "  257 for\n",
      "  244 not\n",
      "  231 that\n",
      "  212 with\n",
      "  202 my\n",
      "  202 very\n",
      "  184 good\n",
      "  176 on\n",
      "  163 you\n",
      "  162 great\n",
      "  158 but\n",
      "  147 have\n",
      "  143 are\n",
      "  142 so\n",
      "  140 movie\n",
      "  137 phone\n",
      "  136 as\n",
      "  120 film\n",
      "  116 be\n",
      "  115 all\n",
      "  111 one\n",
      "  111 its\n",
      "  109 had\n",
      "  103 at\n",
      "  100 place\n",
      "   98 food\n",
      "   95 like\n",
      "   94 were\n",
      "   90 an\n",
      "   89 just\n",
      "   86 there\n",
      "   84 service\n",
      "   84 time\n",
      "   83 if\n",
      "   82 we\n",
      "   79 bad\n",
      "   79 really\n",
      "   78 out\n",
      "   76 they\n",
      "   76 from\n",
      "   75 would\n",
      "   71 well\n",
      "   69 has\n",
      "   69 about\n",
      "   66 your\n",
      "   65 dont\n",
      "   64 only\n",
      "   63 even\n",
      "   63 ever\n",
      "   63 best\n",
      "   62 by\n",
      "   62 back\n",
      "   62 or\n",
      "   59 here\n",
      "   57 also\n",
      "   57 will\n",
      "   56 no\n",
      "   54 up\n",
      "   53 go\n",
      "   52 than\n",
      "   51 quality\n",
      "   51 when\n",
      "   51 love\n",
      "   50 me\n",
      "   50 what\n",
      "   49 can\n",
      "   49 he\n",
      "   48 made\n",
      "   48 more\n",
      "   47 product\n",
      "   47 because\n",
      "   47 excellent\n",
      "   45 im\n",
      "   45 better\n",
      "   45 which\n",
      "   44 recommend\n",
      "   44 some\n",
      "   42 work\n",
      "   42 could\n",
      "   42 ive\n",
      "   42 get\n",
      "   42 too\n",
      "   42 works\n",
      "   42 his\n",
      "   41 been\n",
      "   41 how\n",
      "   41 who\n",
      "   41 our\n",
      "   40 use\n",
      "   40 did\n",
      "   40 after\n",
      "   40 much\n",
      "   40 again\n",
      "   39 headset\n",
      "   39 sound\n",
      "   39 make\n",
      "   39 do\n",
      "   39 nice\n",
      "   38 them\n",
      "   38 any\n",
      "   38 see\n",
      "   37 didnt\n",
      "   37 first\n",
      "   37 other\n",
      "   37 their\n",
      "   36 most\n",
      "   36 way\n",
      "   35 still\n",
      "   35 never\n",
      "   35 think\n",
      "   34 now\n",
      "   33 battery\n",
      "   32 disappointed\n",
      "   31 little\n",
      "   31 pretty\n",
      "   31 every\n",
      "   30 price\n",
      "   30 characters\n",
      "   30 acting\n",
      "   29 enough\n",
      "   29 worst\n",
      "   29 ear\n",
      "   29 being\n",
      "   28 does\n",
      "   28 case\n",
      "   28 movies\n",
      "   27 waste\n",
      "   27 off\n",
      "   27 these\n",
      "   27 right\n",
      "   27 say\n",
      "   27 came\n",
      "   27 got\n",
      "   27 am\n",
      "   27 two\n",
      "   27 amazing\n",
      "   26 thing\n",
      "   26 over\n",
      "   26 money\n",
      "   26 going\n",
      "   26 definitely\n",
      "   25 then\n",
      "   25 everything\n",
      "   25 minutes\n",
      "   25 her\n",
      "   24 doesnt\n",
      "   24 know\n",
      "   24 real\n",
      "   24 us\n",
      "   24 terrible\n",
      "   24 far\n",
      "   24 wonderful\n",
      "   23 cant\n",
      "   23 into\n",
      "   23 poor\n",
      "   23 many\n",
      "   23 new\n",
      "   23 look\n",
      "   23 give\n",
      "   23 people\n",
      "   23 wont\n",
      "   22 both\n",
      "   22 piece\n",
      "   22 while\n",
      "   22 few\n",
      "   22 friendly\n",
      "   22 plot\n",
      "   22 story\n",
      "   22 films\n",
      "   22 restaurant\n",
      "   21 anyone\n",
      "   21 worth\n",
      "   21 nothing\n",
      "   21 probably\n",
      "   21 experience\n",
      "   21 seen\n",
      "   20 times\n",
      "   20 highly\n",
      "   19 around\n",
      "   19 take\n",
      "   19 wasnt\n",
      "   19 screen\n",
      "   19 life\n",
      "   19 want\n",
      "   19 years\n",
      "   19 script\n",
      "   18 since\n",
      "   18 item\n",
      "   18 used\n",
      "   18 stars\n",
      "   18 quite\n",
      "   18 fine\n",
      "   18 easy\n",
      "   18 absolutely\n",
      "   18 loved\n",
      "   18 watching\n",
      "   18 delicious\n",
      "   17 horrible\n",
      "   17 went\n",
      "   17 worked\n",
      "   17 cool\n",
      "   17 night\n",
      "   17 find\n",
      "   17 totally\n",
      "   17 said\n",
      "   17 lot\n",
      "   17 happy\n",
      "   17 another\n",
      "   17 actors\n",
      "   17 show\n",
      "   17 staff\n",
      "   16 always\n",
      "   16 same\n",
      "   16 down\n",
      "   16 big\n",
      "   16 those\n",
      "   16 thought\n",
      "   16 long\n",
      "   16 she\n",
      "   16 should\n",
      "   16 such\n",
      "   16 ordered\n",
      "   16 family\n",
      "   16 awesome\n",
      "   16 character\n",
      "   16 eat\n",
      "   16 vegas\n",
      "   15 makes\n",
      "   15 things\n",
      "   15 before\n",
      "   15 awful\n",
      "   15 end\n",
      "   15 stupid\n",
      "   15 car\n",
      "   15 buy\n",
      "   15 talk\n",
      "   15 day\n",
      "   15 kind\n",
      "   15 though\n",
      "   15 small\n",
      "   15 music\n",
      "   15 funny\n",
      "   15 cast\n",
      "   15 perfect\n",
      "   14 impressed\n",
      "   14 actually\n",
      "   14 camera\n",
      "   14 comfortable\n",
      "   14 old\n",
      "   14 last\n",
      "   14 charger\n",
      "   14 part\n",
      "   14 couldnt\n",
      "   14 found\n",
      "   14 between\n",
      "   14 almost\n",
      "   14 fantastic\n",
      "   14 beautiful\n",
      "   14 scenes\n",
      "   14 watch\n",
      "   13 try\n",
      "   13 gets\n",
      "   13 slow\n",
      "   13 however\n",
      "   13 must\n",
      "   13 expect\n",
      "   13 black\n",
      "   13 come\n",
      "   13 each\n",
      "   13 line\n",
      "   13 especially\n",
      "   13 without\n",
      "   13 job\n",
      "   13 interesting\n",
      "   13 chicken\n",
      "   13 steak\n",
      "   12 charge\n",
      "   12 customer\n",
      "   12 bought\n",
      "   12 bluetooth\n",
      "   12 fit\n",
      "   12 $\n",
      "   12 anything\n",
      "   12 left\n",
      "   12 call\n",
      "   12 year\n",
      "   12 having\n",
      "   12 looks\n",
      "   12 calls\n",
      "   12 sure\n",
      "   12 cheap\n",
      "   12 id\n",
      "   12 problems\n",
      "   12 next\n",
      "   12 reception\n",
      "   12 performance\n",
      "   12 least\n",
      "   12 full\n",
      "   12 overall\n",
      "   12 perfectly\n",
      "   12 fast\n",
      "   12 order\n",
      "   12 hour\n",
      "   12 dialogue\n",
      "   12 man\n",
      "   12 pizza\n",
      "   12 sushi\n",
      "   11 disappointing\n",
      "   11 super\n",
      "   11 everyone\n",
      "   11 felt\n",
      "   11 put\n",
      "   11 plug\n",
      "   11 huge\n",
      "   11 bit\n",
      "   11 problem\n",
      "   11 clear\n",
      "   11 cell\n",
      "   11 ill\n",
      "   11 feel\n",
      "   11 working\n",
      "   11 barely\n",
      "   11 hear\n",
      "   11 may\n",
      "   11 where\n",
      "   11 away\n",
      "   11 hard\n",
      "   11 white\n",
      "   11 coming\n",
      "   11 different\n",
      "   11 bland\n",
      "   11 s\n",
      "   11 done\n",
      "   11 atmosphere\n",
      "   11 scene\n",
      "   11 ending\n",
      "   11 worse\n",
      "   11 special\n",
      "   11 liked\n",
      "   11 enjoyed\n",
      "   11 fresh\n",
      "   11 flavor\n",
      "   11 server\n",
      "   10 low\n",
      "   10 tried\n",
      "   10 getting\n",
      "   10 rather\n",
      "   10 deal\n",
      "   10 soon\n",
      "   10 once\n",
      "   10 using\n",
      "   10 started\n",
      "   10 tell\n",
      "   10 return\n",
      "   10 sucked\n",
      "   10 received\n",
      "   10 understand\n",
      "   10 avoid\n",
      "   10 through\n",
      "   10 thats\n",
      "   10 simply\n",
      "   10 play\n",
      "   10 incredible\n",
      "   10 enjoy\n",
      "   10 whole\n",
      "   10 gave\n",
      "   10 fact\n",
      "   10 saw\n",
      "   10 why\n",
      "   10 writing\n",
      "   10 art\n",
      "   10 buffet\n",
      "   10 salad\n",
      "   10 wait\n",
      "   10 burger\n",
      "   10 menu\n",
      "    9 either\n",
      "    9 three\n",
      "    9 stay\n",
      "    9 disappointment\n",
      "    9 believe\n",
      "    9 hours\n",
      "    9 unfortunately\n",
      "    9 looking\n",
      "    9 completely\n",
      "    9 crap\n",
      "    9 wanted\n",
      "    9 design\n",
      "    9 fits\n",
      "    9 own\n",
      "    9 easily\n",
      "    9 purchase\n",
      "    9 care\n",
      "    9 less\n",
      "    9 kept\n",
      "    9 face\n",
      "    9 truly\n",
      "    9 need\n",
      "    9 short\n",
      "    9 several\n",
      "    9 fun\n",
      "    9 mediocre\n",
      "    9 recommended\n",
      "    9 pleased\n",
      "    9 certainly\n",
      "    9 took\n",
      "    9 during\n",
      "    9 effects\n",
      "    9 game\n",
      "    9 watched\n",
      "    9 second\n",
      "    9 kids\n",
      "    9 feeling\n",
      "    9 director\n",
      "    9 taste\n",
      "    9 waited\n",
      "    9 prices\n",
      "    9 meal\n",
      "    8 wear\n",
      "    8 months\n",
      "    8 junk\n",
      "    8 plus\n",
      "    8 seriously\n",
      "    8 dropped\n",
      "    8 simple\n",
      "    8 motorola\n",
      "    8 strong\n",
      "    8 bar\n",
      "    8 theres\n",
      "    8 voice\n",
      "    8 guess\n",
      "    8 expected\n",
      "    8 together\n",
      "    8 cannot\n",
      "    8 hate\n",
      "    8 set\n",
      "    8 high\n",
      "    8 pictures\n",
      "    8 couple\n",
      "    8 trying\n",
      "    8 star\n",
      "    8 quick\n",
      "    8 seems\n",
      "    8 inside\n",
      "    8 friends\n",
      "    8 someone\n",
      "    8 yet\n",
      "    8 tv\n",
      "    8 predictable\n",
      "    8 wasted\n",
      "    8 half\n",
      "    8 amount\n",
      "    8 spot\n",
      "    8 rude\n",
      "    8 meat\n",
      "    8 selection\n",
      "    8 sauce\n",
      "    8 clean\n",
      "    8 overpriced\n",
      "    8 table\n",
      "    8 lunch\n",
      "    8 breakfast\n",
      "    7 oh\n",
      "    7 mention\n",
      "    7 color\n",
      "    7 useless\n",
      "    7 buttons\n",
      "    7 store\n",
      "    7 point\n",
      "    7 house\n",
      "    7 important\n",
      "    7 keep\n",
      "    7 might\n",
      "    7 wrong\n",
      "    7 comes\n",
      "    7 told\n",
      "    7 sucks\n",
      "    7 phones\n",
      "    7 software\n",
      "    7 volume\n",
      "    7 check\n",
      "    7 lost\n",
      "    7 pay\n",
      "    7 days\n",
      "    7 turn\n",
      "    7 cable\n",
      "    7 although\n",
      "    7 audio\n",
      "    7 review\n",
      "    7 original\n",
      "    7 device\n",
      "    7 area\n",
      "    7 under\n",
      "    7 arrived\n",
      "    7 wouldnt\n",
      "    7 else\n",
      "    7 hands\n",
      "    7 hot\n",
      "    7 top\n",
      "    7 )\n",
      "    7 glad\n",
      "    7 jabra\n",
      "    7 brilliant\n",
      "    7 outside\n",
      "    7 mess\n",
      "    7 human\n",
      "    7 actor\n",
      "    7 playing\n",
      "    7 waitress\n",
      "    7 itself\n",
      "    7 style\n",
      "    7 single\n",
      "    7 twice\n",
      "    7 cinema\n",
      "    7 played\n",
      "    7 hope\n",
      "    7 cold\n",
      "    7 tasted\n",
      "    7 dishes\n",
      "    7 attentive\n",
      "    7 waiter\n",
      "    7 fries\n",
      "    6 none\n",
      "    6 ears\n",
      "    6 signal\n",
      "    6 difficult\n",
      "    6 within\n",
      "    6 weak\n",
      "    6 dead\n",
      "    6 nokia\n",
      "    6 zero\n",
      "    6 week\n",
      "    6 perhaps\n",
      "    6 extra\n",
      "    6 plastic\n",
      "    6 side\n",
      "    6 headsets\n",
      "    6 light\n",
      "    6 home\n",
      "    6 mind\n",
      "    6 obviously\n",
      "    6 series\n",
      "    6 walked\n",
      "    6 word\n",
      "    6 garbage\n",
      "    6 elsewhere\n",
      "    6 decent\n",
      "    6 places\n",
      "    6 room\n",
      "    6 amazon\n",
      "    6 extremely\n",
      "    6 free\n",
      "    6 charm\n",
      "    6 ago\n",
      "    6 authentic\n",
      "    6 exactly\n",
      "    6 range\n",
      "    6 whether\n",
      "    6 entire\n",
      "    6 joy\n",
      "    6 reasonable\n",
      "    6 setting\n",
      "    6 boring\n",
      "    6 horror\n",
      "    6 lines\n",
      "    6 world\n",
      "    6 directing\n",
      "    6 considering\n",
      "    6 action\n",
      "    6 seeing\n",
      "    6 throughout\n",
      "    6 believable\n",
      "    6 him\n",
      "    6 looked\n",
      "    6 drive\n",
      "    6 john\n",
      "    6 possible\n",
      "    6 today\n",
      "    6 history\n",
      "    6 heart\n",
      "    6 sweet\n",
      "    6 served\n",
      "    6 dish\n",
      "    6 spicy\n",
      "    6 eating\n",
      "    6 cooked\n",
      "    6 tasty\n",
      "    6 management\n",
      "    6 sandwich\n",
      "    6 town\n",
      "    5 charging\n",
      "    5 lacking\n",
      "    5 basically\n",
      "    5 feels\n",
      "    5 poorly\n",
      "    5 hold\n",
      "    5 blue\n",
      "    5 %\n",
      "    5 literally\n",
      "    5 feature\n",
      "    5 picture\n",
      "    5 buying\n",
      "    5 headphones\n",
      "    5 until\n",
      "    5 unless\n",
      "    5 waiting\n",
      "    5 broke\n",
      "    5 verizon\n",
      "    5 sometimes\n",
      "    5 mostly\n",
      "    5 company\n",
      "    5 happened\n",
      "    5 unit\n",
      "    5 whatsoever\n",
      "    5 computer\n",
      "    5 bother\n",
      "    5 belt\n",
      "    5 above\n",
      "    5 bars\n",
      "    5 internet\n",
      "    5 rare\n",
      "    5 reason\n",
      "    5 wife\n",
      "    5 thin\n",
      "    5 ended\n",
      "    5 given\n",
      "    5 needed\n",
      "    5 quickly\n",
      "    5 holds\n",
      "    5 sides\n",
      "    5 helpful\n",
      "    5 large\n",
      "    5 others\n",
      "    5 wall\n",
      "    5 superb\n",
      "    5 finally\n",
      "    5 value\n",
      "    5 offers\n",
      "    5 priced\n",
      "    5 cases\n",
      "    5 leather\n",
      "    5 instead\n",
      "    5 youd\n",
      "    5 ask\n",
      "    5 owned\n",
      "    5 solid\n",
      "    5 clever\n",
      "    5 living\n",
      "    5 casting\n",
      "    5 whatever\n",
      "    5 sick\n",
      "    5 drama\n",
      "    5 rent\n",
      "    5 plain\n",
      "    5 lacks\n",
      "    5 hitchcock\n",
      "    5 annoying\n",
      "    5 written\n",
      "    5 girl\n",
      "    5 direction\n",
      "    5 pathetic\n",
      "    5 damn\n",
      "    5 average\n",
      "    5 leave\n",
      "    5 fish\n",
      "    5 myself\n",
      "    5 lead\n",
      "    5 empty\n",
      "    5 eyes\n",
      "    5 business\n",
      "    5 bring\n",
      "    5 close\n",
      "    5 (and\n",
      "    5 cinematography\n",
      "    5 child\n",
      "    5 running\n",
      "    5 lovely\n",
      "    5 portrayal\n",
      "    5 subtle\n",
      "    5 sad\n",
      "    5 silent\n",
      "    5 warm\n",
      "    5 eaten\n",
      "    5 servers\n",
      "    5 shrimp\n",
      "    5 fried\n",
      "    5 anytime\n",
      "    5 beer\n",
      "    5 pasta\n",
      "    5 pho\n",
      "    5 seated\n",
      "    5 chips\n",
      "    5 seafood\n",
      "    5 portions\n",
      "    5 beef\n",
      "    5 ice\n",
      "    5 tender\n",
      "    4 weird\n",
      "    4 says\n",
      "    4 failed\n",
      "    4 red\n",
      "    4 mistake\n",
      "    4 advise\n",
      "    4 charged\n",
      "    4 unreliable\n",
      "    4 ability\n",
      "    4 isnt\n",
      "    4 break\n",
      "    4 plantronics\n",
      "    4 bt\n",
      "    4 clip\n",
      "    4 pair\n",
      "    4 likes\n",
      "    4 sending\n",
      "    4 despite\n",
      "    4 checked\n",
      "    4 turns\n",
      "    4 something\n",
      "    4 shipping\n",
      "    4 later\n",
      "    4 reading\n",
      "    4 front\n",
      "    4 player\n",
      "    4 form\n",
      "    4 goes\n",
      "    4 genuine\n",
      "    4 pull\n",
      "    4 trash\n",
      "    4 rated\n",
      "    4 impressive\n",
      "    4 fall\n",
      "    4 below\n",
      "    4 stuff\n",
      "    4 please\n",
      "    4 previous\n",
      "    4 clearly\n",
      "    4 third\n",
      "    4 gotten\n",
      "    4 missed\n",
      "    4 connection\n",
      "    4 lasts\n",
      "    4 disgusting\n",
      "    4 book\n",
      "    4 fairly\n",
      "    4 replace\n",
      "    4 able\n",
      "    4 moving\n",
      "    4 lightweight\n",
      "    4 recently\n",
      "    4 dirty\n",
      "    4 seem\n",
      "    4 costs\n",
      "    4 appearance\n",
      "    4 cute\n",
      "    4 sturdy\n",
      "    4 type\n",
      "    4 features\n",
      "    4 crisp\n",
      "    4 theyre\n",
      "    4 doing\n",
      "    4 hand\n",
      "    4 adorable\n",
      "    4 keyboard\n",
      "    4 strip\n",
      "    4 roles\n",
      "    4 total\n",
      "    4 youll\n",
      "    4 passed\n",
      "    4 beat\n",
      "    4 thumbs\n",
      "    4 graphics\n",
      "    4 incredibly\n",
      "    4 making\n",
      "    4 favorite\n",
      "    4 shots\n",
      "    4 wish\n",
      "    4 premise\n",
      "    4 theater\n",
      "    4 idea\n",
      "    4 complete\n",
      "    4 ridiculous\n",
      "    4 fails\n",
      "    4 editing\n",
      "    4 wow\n",
      "    4 seemed\n",
      "    4 torture\n",
      "    4 pretentious\n",
      "    4 visual\n",
      "    4 usual\n",
      "    4 depth\n",
      "    4 utterly\n",
      "    4 towards\n",
      "    4 beyond\n",
      "    4 minute\n",
      "    4 sense\n",
      "    4 stories\n",
      "    4 unbelievable\n",
      "    4 particularly\n",
      "    4 shot\n",
      "    4 space\n",
      "    4 mean\n",
      "    4 imagination\n",
      "    4 words\n",
      "    4 chemistry\n",
      "    4 consider\n",
      "    4 already\n",
      "    4 suspense\n",
      "    4 arent\n",
      "    4 heard\n",
      "    4 gives\n",
      "    4 created\n",
      "    4 rate\n",
      "    4 reviews\n",
      "    4 fan\n",
      "    4 songs\n",
      "    4 often\n",
      "    4 budget\n",
      "    4 flick\n",
      "    4 lots\n",
      "    4 rolls\n",
      "    4 Â–\n",
      "    4 guy\n",
      "    4 joke\n",
      "    4 flat\n",
      "    4 attention\n",
      "    4 directors\n",
      "    4 rest\n",
      "    4 drago\n",
      "    4 youre\n",
      "    4 performances\n",
      "    4 dry\n",
      "    4 list\n",
      "    4 audience\n",
      "    4 ten\n",
      "    4 +\n",
      "    4 beginning\n",
      "    4 memorable\n",
      "    4 terrific\n",
      "    4 nasty\n",
      "    4 hilarious\n",
      "    4 role\n",
      "    4 enjoyable\n",
      "    4 deserves\n",
      "    4 appreciate\n",
      "    4 massive\n",
      "    4 sat\n",
      "    4 vibe\n",
      "    4 location\n",
      "    4 tables\n",
      "    4 ambiance\n",
      "    4 live\n",
      "    4 husband\n",
      "    4 drinks\n",
      "    4 tasteless\n",
      "    4 establishment\n",
      "    4 asked\n",
      "    4 needs\n",
      "    4 folks\n",
      "    4 dining\n",
      "    4 salmon\n",
      "    4 dinner\n",
      "    4 tacos\n",
      "    4 dessert\n",
      "    4 steaks\n",
      "    4 bread\n",
      "    4 bacon\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# printing out the frequency of word\n",
    "sorted_tokens = list(sorted(tok_count_dict, key=tok_count_dict.get, reverse=True))\n",
    "# filter out words that only appear less than 4 times\n",
    "vocab_list = [w for w in sorted_tokens if tok_count_dict[w] >= 4]\n",
    "\n",
    "for w in vocab_list:\n",
    "    print(\"%5d %s\" % (tok_count_dict[w], w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7aaf2-d607-4bfb-982b-ab0107931c65",
   "metadata": {},
   "source": [
    "## Step3: Feature Extration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bccb3a1-427e-4629-9412-4510c85a5a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the original vacabulary list:\n",
      "4747\n",
      "The size of the first vacabulary list:\n",
      "55\n",
      "The size of the second vacabulary list:\n",
      "4624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\globa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sorted_tokens = list(sorted(tok_count_dict, key=tok_count_dict.get, reverse=True))\n",
    "\n",
    "# filter out the vocabulary that appears in just one time\n",
    "vocab_list_2 = [w for w in sorted_tokens if tok_count_dict[w] >= 1]\n",
    "\n",
    "# for first attempt, we tried to manualy built vocabulary list\n",
    "vocab_list = ['good', 'great', 'like', 'bad', 'best', 'love', 'excellent', 'better', 'recommend',\n",
    "              'nice', 'disappointed', 'pretty', 'worst', 'waste', 'amazing', 'terrible', 'wonderful',\n",
    "              'poor', 'friendly', 'loved', 'delicious', 'horrible', 'cool', 'happy', 'awesome', 'awful',\n",
    "              'stupid', 'perfect', 'impressed', 'comfortable', 'fantastic', 'beautiful', 'interesting',\n",
    "              'perfectly', 'disappointing', 'super', 'fast', 'problem', 'bland', 'worse', 'enjoyed', 'fresh',\n",
    "              'avoid', 'incredible', 'didn\\'t work', 'weird', 'useless', 'enjoy',\n",
    "              'sucked', 'disappointment', 'unfortunately', 'mediocre', 'recommended', 'pleased', 'junk']\n",
    "\n",
    "# for second try, we tried to import stopword from Natural Language Tool Kit\n",
    "nltk.download('stopwords')\n",
    "useless = stopwords.words('english')\n",
    "filtered_words = [word for word in vocab_list_2 if not word in useless]\n",
    "\n",
    "# print out sizes\n",
    "print(\"The size of the original vacabulary list:\")\n",
    "print(len(sorted_tokens))\n",
    "\n",
    "print(\"The size of the first vacabulary list:\")\n",
    "print(len(vocab_list))\n",
    "\n",
    "print(\"The size of the second vacabulary list:\")\n",
    "print(len(filtered_words))\n",
    "\n",
    "vocab_dict = dict()\n",
    "\n",
    "for vocab_id, tok in enumerate(filtered_words):\n",
    "    vocab_dict[tok] = vocab_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b30cc94-3b80-4826-9de4-28f491eafad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 4624)\n",
      "(2400, 4624)\n"
     ]
    }
   ],
   "source": [
    "bow_preprocessor = CountVectorizer(binary=False, vocabulary=vocab_dict)\n",
    "\n",
    "bow_preprocessor.fit(tr_text_list)\n",
    "\n",
    "sparse_arr = bow_preprocessor.transform(tr_text_list)\n",
    "\n",
    "print(sparse_arr.shape)\n",
    "\n",
    "dense_arr_NV = sparse_arr.toarray()\n",
    "\n",
    "print(dense_arr_NV.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d750d6",
   "metadata": {},
   "source": [
    "## Step4: Create Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd8c78c-a98b-4e55-b2a2-879b318effa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bow_classifier_pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('my_bow_feature_extractor',\n",
    "     CountVectorizer(min_df=1, max_df=1.0, ngram_range=(1, 1), vocabulary=vocab_dict, binary=False)),\n",
    "    ('my_classifier', sklearn.linear_model.LogisticRegression(C=1.0, max_iter=1000, random_state=101))\n",
    "])\n",
    "\n",
    "my_parameter_grid_by_name = dict()\n",
    "my_parameter_grid_by_name['my_bow_feature_extractor__min_df'] = [1, 2, 4]\n",
    "my_parameter_grid_by_name['my_classifier__C'] = np.logspace(-5, 5, 11)\n",
    "\n",
    "my_scoring_metric_name = 'accuracy'\n",
    "y_true = np.ravel(y_train_df)\n",
    "# print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad477a1e",
   "metadata": {},
   "source": [
    "## Step5: Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "319c46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_my_bow_feature_extractor__min_df param_my_classifier__C  \\\n",
      "0                                       1                0.00001   \n",
      "1                                       1                 0.0001   \n",
      "2                                       1                  0.001   \n",
      "3                                       1                   0.01   \n",
      "4                                       1                    0.1   \n",
      "5                                       1                    1.0   \n",
      "6                                       1                   10.0   \n",
      "7                                       1                  100.0   \n",
      "8                                       1                 1000.0   \n",
      "9                                       1                10000.0   \n",
      "10                                      1               100000.0   \n",
      "11                                      2                0.00001   \n",
      "12                                      2                 0.0001   \n",
      "13                                      2                  0.001   \n",
      "14                                      2                   0.01   \n",
      "15                                      2                    0.1   \n",
      "16                                      2                    1.0   \n",
      "17                                      2                   10.0   \n",
      "18                                      2                  100.0   \n",
      "19                                      2                 1000.0   \n",
      "20                                      2                10000.0   \n",
      "21                                      2               100000.0   \n",
      "22                                      4                0.00001   \n",
      "23                                      4                 0.0001   \n",
      "24                                      4                  0.001   \n",
      "25                                      4                   0.01   \n",
      "26                                      4                    0.1   \n",
      "27                                      4                    1.0   \n",
      "28                                      4                   10.0   \n",
      "29                                      4                  100.0   \n",
      "30                                      4                 1000.0   \n",
      "31                                      4                10000.0   \n",
      "32                                      4               100000.0   \n",
      "\n",
      "    split0_test_score  rank_test_score  \n",
      "0            0.490687               25  \n",
      "1            0.490687               25  \n",
      "2            0.490687               25  \n",
      "3            0.536671               22  \n",
      "4            0.736321                7  \n",
      "5            0.752619                1  \n",
      "6            0.742142                4  \n",
      "7            0.715949               10  \n",
      "8            0.702561               13  \n",
      "9            0.690338               16  \n",
      "10           0.688009               19  \n",
      "11           0.490687               25  \n",
      "12           0.490687               25  \n",
      "13           0.490687               25  \n",
      "14           0.536671               22  \n",
      "15           0.736321                7  \n",
      "16           0.752619                1  \n",
      "17           0.742142                4  \n",
      "18           0.715949               10  \n",
      "19           0.702561               13  \n",
      "20           0.690338               16  \n",
      "21           0.688009               19  \n",
      "22           0.490687               25  \n",
      "23           0.490687               25  \n",
      "24           0.490687               25  \n",
      "25           0.536671               22  \n",
      "26           0.736321                7  \n",
      "27           0.752619                1  \n",
      "28           0.742142                4  \n",
      "29           0.715949               10  \n",
      "30           0.702561               13  \n",
      "31           0.690338               16  \n",
      "32           0.688009               19  \n"
     ]
    }
   ],
   "source": [
    "prng = np.random.RandomState(0)\n",
    "\n",
    "valid_ids = prng.choice(np.arange(N), size=800)\n",
    "\n",
    "valid_indicators_N = np.zeros(N)\n",
    "valid_indicators_N[valid_ids] = -1\n",
    "\n",
    "my_splitter = sklearn.model_selection.PredefinedSplit(valid_indicators_N)\n",
    "\n",
    "grid_searcher = sklearn.model_selection.GridSearchCV(\n",
    "    my_bow_classifier_pipeline,\n",
    "    my_parameter_grid_by_name,\n",
    "    scoring=my_scoring_metric_name,\n",
    "    cv=my_splitter,\n",
    "    refit=False)\n",
    "\n",
    "grid_searcher.fit(tr_text_list, y_true)\n",
    "\n",
    "gsearch_results_df = pd.DataFrame(grid_searcher.cv_results_).copy()\n",
    "\n",
    "param_keys = ['param_my_bow_feature_extractor__min_df', 'param_my_classifier__C']\n",
    "\n",
    "# Rearrange row order so it is easy to skim\n",
    "gsearch_results_df.sort_values(param_keys, inplace=True)\n",
    "\n",
    "var = gsearch_results_df[param_keys + ['split0_test_score', 'rank_test_score']]\n",
    "\n",
    "print(var)\n",
    "\n",
    "# y_test_pred = np.ravel(test_text_list)\n",
    "# my_bow_classifier_pipeline.fit(tr_text_list, y_true)\n",
    "# result = my_bow_classifier_pipeline.predict(y_test_pred)\n",
    "#\n",
    "# # print(result[:, np.newaxis])\n",
    "\n",
    "\n",
    "# print(yhat_tr_N[:,np.newaxis])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917db808",
   "metadata": {},
   "source": [
    "## Step6: Hyperparameter Selection for Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f92137a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_tr_bce_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39mlog10(my_parameter_grid_by_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_classifier__C\u001b[39m\u001b[38;5;124m'\u001b[39m]), cv_tr_bce_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain BCE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39mlog10(my_parameter_grid_by_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_classifier__C\u001b[39m\u001b[38;5;124m'\u001b[39m]), cv_va_bce_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrs-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid BCE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# plt.plot(np.log10(C_grid), tr_err_list, 'b:', label='train err')\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# plt.plot(np.log10(C_grid), va_err_list, 'r:', label='valid err')\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv_tr_bce_list' is not defined"
     ]
    }
   ],
   "source": [
    "tr_bce_list = list()\n",
    "va_bce_list = list()\n",
    "te_bce_list = list()\n",
    "\n",
    "my_tr_err = sklearn.metrics.zero_one_loss(y_tr_M, yproba1_tr_M >= 0.5)\n",
    "my_va_err = sklearn.metrics.zero_one_loss(y_va_N, yproba1_va_N >= 0.5)\n",
    "tr_err_list.append(my_tr_err)\n",
    "va_err_list.append(my_va_err)\n",
    "\n",
    "plt.plot(np.log10(my_parameter_grid_by_name['my_classifier__C']), cv_tr_bce_list, 'bs-', label='train BCE')\n",
    "plt.plot(np.log10(my_parameter_grid_by_name['my_classifier__C']), cv_va_bce_list, 'rs-', label='valid BCE')\n",
    "\n",
    "# need to findout bce list\n",
    "\n",
    "plt.ylabel('error')\n",
    "plt.xlabel(\"$\\log_{10} C$\");\n",
    "plt.legend(bbox_to_anchor=(1.5, 0.5)) # make legend outside plot\n",
    "plt.ylim([0, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48280e1",
   "metadata": {},
   "source": [
    "## Step7: Using the Best Pipeline to Predict the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832dcafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9633333333333334\n",
      "[0.62046604 0.49903021 0.04047966 0.08190969 0.43429751 0.234294\n",
      " 0.18919695 0.21753659 0.45236634 0.48948749 0.4700432  0.49955902\n",
      " 0.06463395 0.23095775 0.12391673 0.22585905 0.01162406 0.12602387\n",
      " 0.30966111 0.46967848 0.32798882 0.80272303 0.34660883 0.63941181\n",
      " 0.18564912 0.28927044 0.11235399 0.41253816 0.30881727 0.3745664\n",
      " 0.33458067 0.37084583 0.28866148 0.18097874 0.41667162 0.48652297\n",
      " 0.46844879 0.35120818 0.79960855 0.40198479 0.08896942 0.16334475\n",
      " 0.70490411 0.14884563 0.49443342 0.20377364 0.54471988 0.2071105\n",
      " 0.01179928 0.30881727 0.12302065 0.15074044 0.82526901 0.74134253\n",
      " 0.29940647 0.25322137 0.14530045 0.03339261 0.64687824 0.04808236\n",
      " 0.13735325 0.29121088 0.12302065 0.08154496 0.32084553 0.0559571\n",
      " 0.20669286 0.89681185 0.19807693 0.56812119 0.24807235 0.41253816\n",
      " 0.15531084 0.02881026 0.32828654 0.30416875 0.2614436  0.04929479\n",
      " 0.35029235 0.73891079 0.22875678 0.12596727 0.26281469 0.15810331\n",
      " 0.01001209 0.20426154 0.1681406  0.16586347 0.1375028  0.37058203\n",
      " 0.31754704 0.2553074  0.52970964 0.07205948 0.75162443 0.08590426\n",
      " 0.27232577 0.26814197 0.27123391 0.77635133 0.98686392 0.58722887\n",
      " 0.52528203 0.84435386 0.45283801 0.97528406 0.88160061 0.96108479\n",
      " 0.26882262 0.47225477 0.87107342 0.31888157 0.91306338 0.2969908\n",
      " 0.78568176 0.67822449 0.77130858 0.94039388 0.6748752  0.43652762\n",
      " 0.04104495 0.59973049 0.69958972 0.93505753 0.92562471 0.9660563\n",
      " 0.65572649 0.78338771 0.98580003 0.59506682 0.99722166 0.73188434\n",
      " 0.9630318  0.99105838 0.99841093 0.58442988 0.09622497 0.89630172\n",
      " 0.77382078 0.94712689 0.94562937 0.21541166 0.74310343 0.9335583\n",
      " 0.78588853 0.70521962 0.22772878 0.34237364 0.96426069 0.90796738\n",
      " 0.99972046 0.9828585  0.49693231 0.92661419 0.601255   0.5680152\n",
      " 0.81600409 0.62023415 0.9815563  0.69542596 0.73947498 0.4409318\n",
      " 0.67850649 0.97083189 0.96934832 0.4776417  0.78078508 0.26900229\n",
      " 0.56570172 0.6015309  0.31573425 0.57266133 0.65627337 0.59883125\n",
      " 0.57143807 0.9892221  0.54368577 0.42015897 0.92247711 0.84929488\n",
      " 0.771584   0.83473094 0.7290911  0.89361271 0.61456499 0.93250224\n",
      " 0.88155084 0.94055325 0.59281618 0.88125342 0.92273526 0.49144974\n",
      " 0.58060707 0.98655021 0.73864816 0.8243141  0.38388484 0.37648239\n",
      " 0.58844243 0.86714817 0.06501367 0.43992438 0.25215703 0.63439627\n",
      " 0.01533356 0.59950072 0.12938353 0.00142359 0.19093125 0.29646391\n",
      " 0.25534656 0.03092594 0.34561006 0.38976786 0.51660338 0.30649385\n",
      " 0.68774367 0.38450682 0.35951155 0.07442352 0.01886179 0.13143026\n",
      " 0.54075913 0.27347527 0.15043416 0.09302811 0.01915864 0.45076204\n",
      " 0.28403453 0.33618012 0.45358995 0.19817012 0.90659231 0.32956077\n",
      " 0.33582238 0.12821757 0.07201392 0.2446358  0.41544376 0.03399994\n",
      " 0.12725878 0.13950699 0.49687841 0.00266502 0.83145305 0.06508428\n",
      " 0.1343207  0.00393673 0.11008447 0.20182744 0.27817068 0.65591893\n",
      " 0.49842324 0.12175731 0.42145567 0.09184675 0.47936388 0.11602519\n",
      " 0.08606686 0.42383821 0.31657746 0.15046217 0.20789656 0.27453113\n",
      " 0.15926793 0.18723241 0.293387   0.11940258 0.09374459 0.35201338\n",
      " 0.08109432 0.29990088 0.7272189  0.23061675 0.16801298 0.14701629\n",
      " 0.45194439 0.17370459 0.3206414  0.44608136 0.37414496 0.27835162\n",
      " 0.42880687 0.35703441 0.39543604 0.13003956 0.57829623 0.14656856\n",
      " 0.14837569 0.65048511 0.11174622 0.18542098 0.1155158  0.08339432\n",
      " 0.23485911 0.12076781 0.10574397 0.3599647  0.06882916 0.72540257\n",
      " 0.63532269 0.75843439 0.94595312 0.76912132 0.62379102 0.96103091\n",
      " 0.45476123 0.1088374  0.54725328 0.5703681  0.51402391 0.51251816\n",
      " 0.34240454 0.52931386 0.20210894 0.68970302 0.42823672 0.87578245\n",
      " 0.51079976 0.8828928  0.86580005 0.9995939  0.8084852  0.74543909\n",
      " 0.64012809 0.8225294  0.99850209 0.55607053 0.9706457  0.52791035\n",
      " 0.97473683 0.37632796 0.52445228 0.29181612 0.4825304  0.99083588\n",
      " 0.91053053 0.82192466 0.48859528 0.48032904 0.36766343 0.35058056\n",
      " 0.7187652  0.48480692 0.94103337 0.70758545 0.41430785 0.74448947\n",
      " 0.08448129 0.80074562 0.57150144 0.23572219 0.74919497 0.64012809\n",
      " 0.81186761 0.31826166 0.89837183 0.58519737 0.97421688 0.9739832\n",
      " 0.35651922 0.92179778 0.34522423 0.48164137 0.72055822 0.72457218\n",
      " 0.97532258 0.9395315  0.3448902  0.85169487 0.29657465 0.74551246\n",
      " 0.54088769 0.43252687 0.45059547 0.46284989 0.56892835 0.2384651\n",
      " 0.65541    0.73774478 0.37133319 0.90617657 0.43430091 0.9825314\n",
      " 0.61832492 0.87892225 0.73194134 0.28102518 0.97260503 0.68981322\n",
      " 0.4137862  0.16748966 0.96143442 0.25493027 0.79710238 0.91375441\n",
      " 0.59492377 0.7559141  0.13802528 0.79358221 0.10843892 0.20365118\n",
      " 0.0109715  0.29352699 0.24869675 0.23603217 0.48689676 0.14951352\n",
      " 0.1318558  0.46601704 0.25259782 0.06686669 0.09720335 0.3714581\n",
      " 0.23714112 0.14310543 0.2508413  0.11060789 0.49371823 0.48996745\n",
      " 0.39091819 0.27881498 0.2508413  0.61346611 0.73078855 0.32682451\n",
      " 0.26921347 0.34015398 0.23677862 0.7334132  0.33677046 0.34971573\n",
      " 0.37543931 0.17573698 0.45092049 0.46998906 0.48694705 0.1181291\n",
      " 0.07665722 0.25794098 0.53455327 0.41738288 0.11096879 0.13063541\n",
      " 0.12436858 0.46044508 0.67022614 0.37874449 0.3631017  0.43326566\n",
      " 0.63980074 0.36291743 0.14006682 0.18668659 0.0734461  0.39306457\n",
      " 0.35363424 0.47365252 0.4759317  0.23758105 0.09787514 0.11849259\n",
      " 0.61323458 0.25065164 0.05568892 0.75360937 0.33016044 0.27555425\n",
      " 0.35004705 0.76993709 0.24311998 0.28639464 0.67159501 0.42298922\n",
      " 0.44071455 0.75363914 0.13364402 0.31054392 0.08861142 0.3053606\n",
      " 0.0579782  0.25415931 0.67288191 0.20663767 0.31267354 0.13659016\n",
      " 0.01410597 0.32192261 0.33539861 0.17294192 0.58384999 0.46060761\n",
      " 0.4337216  0.7650264  0.49209521 0.44126814 0.15587192 0.13988008\n",
      " 0.31094698 0.6032718  0.97707465 0.74443363 0.80272303 0.32909962\n",
      " 0.36103623 0.59890148 0.93455081 0.84008029 0.96610711 0.25375705\n",
      " 0.59789114 0.92022785 0.70208323 0.73751286 0.43587523 0.70825814\n",
      " 0.79914538 0.81456669 0.54358015 0.54692168 0.55726503 0.864309\n",
      " 0.85068915 0.10696211 0.71813482 0.9544481  0.03910179 0.70697651\n",
      " 0.77068758 0.99872591 0.47949    0.93402913 0.41339786 0.82133953\n",
      " 0.41740199 0.42319801 0.53564372 0.3378565  0.7056548  0.96796698\n",
      " 0.68417646 0.61967975 0.94283122 0.81850237 0.64089465 0.70138018\n",
      " 0.82103299 0.53675975 0.93158637 0.9907013  0.54718587 0.11822641\n",
      " 0.8579428  0.95496628 0.82182087 0.95924859 0.81694647 0.82429856\n",
      " 0.5622547  0.44330503 0.57651775 0.83534051 0.98768016 0.91108999\n",
      " 0.97920196 0.59120927 0.34391703 0.87072422 0.83219968 0.42754028\n",
      " 0.91312219 0.50437752 0.31084482 0.20526461 0.49054681 0.27935579\n",
      " 0.66402507 0.82318449 0.71494471 0.83566082 0.93526987 0.26367965\n",
      " 0.92882484 0.45543673 0.58570918 0.97410666 0.78256826 0.85326627\n",
      " 0.92037    0.44002179 0.14329537 0.47750978 0.82629451 0.52622467\n",
      " 0.85867683 0.96604817 0.6372429  0.83815895 0.99722145 0.71413627]\n",
      "[0.62046604 0.49903021 0.04047966 0.08190969 0.43429751 0.234294\n",
      " 0.18919695 0.21753659 0.45236634 0.48948749 0.4700432  0.49955902\n",
      " 0.06463395 0.23095775 0.12391673 0.22585905 0.01162406 0.12602387\n",
      " 0.30966111 0.46967848 0.32798882 0.80272303 0.34660883 0.63941181\n",
      " 0.18564912 0.28927044 0.11235399 0.41253816 0.30881727 0.3745664\n",
      " 0.33458067 0.37084583 0.28866148 0.18097874 0.41667162 0.48652297\n",
      " 0.46844879 0.35120818 0.79960855 0.40198479 0.08896942 0.16334475\n",
      " 0.70490411 0.14884563 0.49443342 0.20377364 0.54471988 0.2071105\n",
      " 0.01179928 0.30881727 0.12302065 0.15074044 0.82526901 0.74134253\n",
      " 0.29940647 0.25322137 0.14530045 0.03339261 0.64687824 0.04808236\n",
      " 0.13735325 0.29121088 0.12302065 0.08154496 0.32084553 0.0559571\n",
      " 0.20669286 0.89681185 0.19807693 0.56812119 0.24807235 0.41253816\n",
      " 0.15531084 0.02881026 0.32828654 0.30416875 0.2614436  0.04929479\n",
      " 0.35029235 0.73891079 0.22875678 0.12596727 0.26281469 0.15810331\n",
      " 0.01001209 0.20426154 0.1681406  0.16586347 0.1375028  0.37058203\n",
      " 0.31754704 0.2553074  0.52970964 0.07205948 0.75162443 0.08590426\n",
      " 0.27232577 0.26814197 0.27123391 0.77635133 0.98686392 0.58722887\n",
      " 0.52528203 0.84435386 0.45283801 0.97528406 0.88160061 0.96108479\n",
      " 0.26882262 0.47225477 0.87107342 0.31888157 0.91306338 0.2969908\n",
      " 0.78568176 0.67822449 0.77130858 0.94039388 0.6748752  0.43652762\n",
      " 0.04104495 0.59973049 0.69958972 0.93505753 0.92562471 0.9660563\n",
      " 0.65572649 0.78338771 0.98580003 0.59506682 0.99722166 0.73188434\n",
      " 0.9630318  0.99105838 0.99841093 0.58442988 0.09622497 0.89630172\n",
      " 0.77382078 0.94712689 0.94562937 0.21541166 0.74310343 0.9335583\n",
      " 0.78588853 0.70521962 0.22772878 0.34237364 0.96426069 0.90796738\n",
      " 0.99972046 0.9828585  0.49693231 0.92661419 0.601255   0.5680152\n",
      " 0.81600409 0.62023415 0.9815563  0.69542596 0.73947498 0.4409318\n",
      " 0.67850649 0.97083189 0.96934832 0.4776417  0.78078508 0.26900229\n",
      " 0.56570172 0.6015309  0.31573425 0.57266133 0.65627337 0.59883125\n",
      " 0.57143807 0.9892221  0.54368577 0.42015897 0.92247711 0.84929488\n",
      " 0.771584   0.83473094 0.7290911  0.89361271 0.61456499 0.93250224\n",
      " 0.88155084 0.94055325 0.59281618 0.88125342 0.92273526 0.49144974\n",
      " 0.58060707 0.98655021 0.73864816 0.8243141  0.38388484 0.37648239\n",
      " 0.58844243 0.86714817 0.06501367 0.43992438 0.25215703 0.63439627\n",
      " 0.01533356 0.59950072 0.12938353 0.00142359 0.19093125 0.29646391\n",
      " 0.25534656 0.03092594 0.34561006 0.38976786 0.51660338 0.30649385\n",
      " 0.68774367 0.38450682 0.35951155 0.07442352 0.01886179 0.13143026\n",
      " 0.54075913 0.27347527 0.15043416 0.09302811 0.01915864 0.45076204\n",
      " 0.28403453 0.33618012 0.45358995 0.19817012 0.90659231 0.32956077\n",
      " 0.33582238 0.12821757 0.07201392 0.2446358  0.41544376 0.03399994\n",
      " 0.12725878 0.13950699 0.49687841 0.00266502 0.83145305 0.06508428\n",
      " 0.1343207  0.00393673 0.11008447 0.20182744 0.27817068 0.65591893\n",
      " 0.49842324 0.12175731 0.42145567 0.09184675 0.47936388 0.11602519\n",
      " 0.08606686 0.42383821 0.31657746 0.15046217 0.20789656 0.27453113\n",
      " 0.15926793 0.18723241 0.293387   0.11940258 0.09374459 0.35201338\n",
      " 0.08109432 0.29990088 0.7272189  0.23061675 0.16801298 0.14701629\n",
      " 0.45194439 0.17370459 0.3206414  0.44608136 0.37414496 0.27835162\n",
      " 0.42880687 0.35703441 0.39543604 0.13003956 0.57829623 0.14656856\n",
      " 0.14837569 0.65048511 0.11174622 0.18542098 0.1155158  0.08339432\n",
      " 0.23485911 0.12076781 0.10574397 0.3599647  0.06882916 0.72540257\n",
      " 0.63532269 0.75843439 0.94595312 0.76912132 0.62379102 0.96103091\n",
      " 0.45476123 0.1088374  0.54725328 0.5703681  0.51402391 0.51251816\n",
      " 0.34240454 0.52931386 0.20210894 0.68970302 0.42823672 0.87578245\n",
      " 0.51079976 0.8828928  0.86580005 0.9995939  0.8084852  0.74543909\n",
      " 0.64012809 0.8225294  0.99850209 0.55607053 0.9706457  0.52791035\n",
      " 0.97473683 0.37632796 0.52445228 0.29181612 0.4825304  0.99083588\n",
      " 0.91053053 0.82192466 0.48859528 0.48032904 0.36766343 0.35058056\n",
      " 0.7187652  0.48480692 0.94103337 0.70758545 0.41430785 0.74448947\n",
      " 0.08448129 0.80074562 0.57150144 0.23572219 0.74919497 0.64012809\n",
      " 0.81186761 0.31826166 0.89837183 0.58519737 0.97421688 0.9739832\n",
      " 0.35651922 0.92179778 0.34522423 0.48164137 0.72055822 0.72457218\n",
      " 0.97532258 0.9395315  0.3448902  0.85169487 0.29657465 0.74551246\n",
      " 0.54088769 0.43252687 0.45059547 0.46284989 0.56892835 0.2384651\n",
      " 0.65541    0.73774478 0.37133319 0.90617657 0.43430091 0.9825314\n",
      " 0.61832492 0.87892225 0.73194134 0.28102518 0.97260503 0.68981322\n",
      " 0.4137862  0.16748966 0.96143442 0.25493027 0.79710238 0.91375441\n",
      " 0.59492377 0.7559141  0.13802528 0.79358221 0.10843892 0.20365118\n",
      " 0.0109715  0.29352699 0.24869675 0.23603217 0.48689676 0.14951352\n",
      " 0.1318558  0.46601704 0.25259782 0.06686669 0.09720335 0.3714581\n",
      " 0.23714112 0.14310543 0.2508413  0.11060789 0.49371823 0.48996745\n",
      " 0.39091819 0.27881498 0.2508413  0.61346611 0.73078855 0.32682451\n",
      " 0.26921347 0.34015398 0.23677862 0.7334132  0.33677046 0.34971573\n",
      " 0.37543931 0.17573698 0.45092049 0.46998906 0.48694705 0.1181291\n",
      " 0.07665722 0.25794098 0.53455327 0.41738288 0.11096879 0.13063541\n",
      " 0.12436858 0.46044508 0.67022614 0.37874449 0.3631017  0.43326566\n",
      " 0.63980074 0.36291743 0.14006682 0.18668659 0.0734461  0.39306457\n",
      " 0.35363424 0.47365252 0.4759317  0.23758105 0.09787514 0.11849259\n",
      " 0.61323458 0.25065164 0.05568892 0.75360937 0.33016044 0.27555425\n",
      " 0.35004705 0.76993709 0.24311998 0.28639464 0.67159501 0.42298922\n",
      " 0.44071455 0.75363914 0.13364402 0.31054392 0.08861142 0.3053606\n",
      " 0.0579782  0.25415931 0.67288191 0.20663767 0.31267354 0.13659016\n",
      " 0.01410597 0.32192261 0.33539861 0.17294192 0.58384999 0.46060761\n",
      " 0.4337216  0.7650264  0.49209521 0.44126814 0.15587192 0.13988008\n",
      " 0.31094698 0.6032718  0.97707465 0.74443363 0.80272303 0.32909962\n",
      " 0.36103623 0.59890148 0.93455081 0.84008029 0.96610711 0.25375705\n",
      " 0.59789114 0.92022785 0.70208323 0.73751286 0.43587523 0.70825814\n",
      " 0.79914538 0.81456669 0.54358015 0.54692168 0.55726503 0.864309\n",
      " 0.85068915 0.10696211 0.71813482 0.9544481  0.03910179 0.70697651\n",
      " 0.77068758 0.99872591 0.47949    0.93402913 0.41339786 0.82133953\n",
      " 0.41740199 0.42319801 0.53564372 0.3378565  0.7056548  0.96796698\n",
      " 0.68417646 0.61967975 0.94283122 0.81850237 0.64089465 0.70138018\n",
      " 0.82103299 0.53675975 0.93158637 0.9907013  0.54718587 0.11822641\n",
      " 0.8579428  0.95496628 0.82182087 0.95924859 0.81694647 0.82429856\n",
      " 0.5622547  0.44330503 0.57651775 0.83534051 0.98768016 0.91108999\n",
      " 0.97920196 0.59120927 0.34391703 0.87072422 0.83219968 0.42754028\n",
      " 0.91312219 0.50437752 0.31084482 0.20526461 0.49054681 0.27935579\n",
      " 0.66402507 0.82318449 0.71494471 0.83566082 0.93526987 0.26367965\n",
      " 0.92882484 0.45543673 0.58570918 0.97410666 0.78256826 0.85326627\n",
      " 0.92037    0.44002179 0.14329537 0.47750978 0.82629451 0.52622467\n",
      " 0.85867683 0.96604817 0.6372429  0.83815895 0.99722145 0.71413627]\n"
     ]
    }
   ],
   "source": [
    "new_pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('my_bow_feature_extractor', CountVectorizer(min_df=1, max_df=1.0, ngram_range=(1, 1), vocabulary=vocab_dict, binary=False)),\n",
    "    ('my_classifier', sklearn.linear_model.LogisticRegression(C=1.0, max_iter=1000, random_state=101))\n",
    "])\n",
    "\n",
    "new_pipeline.fit(tr_text_list, y_true)\n",
    "yhat_tr_N = new_pipeline.predict(tr_text_list)\n",
    "acc = np.mean(y_true == yhat_tr_N)\n",
    "print(acc)\n",
    "\n",
    "yhat_test_N = new_pipeline.predict_proba(test_text_list)\n",
    "\n",
    "\n",
    "float_y_test = yhat_test_N[:,1]\n",
    "\n",
    "print(float_y_test)\n",
    "\n",
    "print(float_y_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74dca7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "\n",
    "def calc_confusion_matrix_for_probas_and_threshold(ytrue_N, yproba1_N, thr):\n",
    "    ytrue_N = np.asarray(ytrue_N, dtype=np.int32)\n",
    "    \n",
    "    # Determine hard predictions given probabilities\n",
    "    yproba1_N = np.asarray(yproba1_N, dtype=np.float64)\n",
    "    yhat_N = np.asarray(yproba1_N >= thr, dtype=np.int32)\n",
    "\n",
    "    cm = sklearn.metrics.confusion_matrix(ytrue_N, yhat_N)\n",
    "    cm_df = pd.DataFrame(data=cm, columns=[0, 1], index=[0, 1])\n",
    "    cm_df.columns.name = 'Predicted'\n",
    "    cm_df.index.name = 'True'\n",
    "    return cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "36b48ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted     0     1\n",
      "True                 \n",
      "0          1164    36\n",
      "1            51  1149\n"
     ]
    }
   ],
   "source": [
    "print(calc_confusion_matrix_for_probas_and_threshold(y_train_df, yhat_tr_N, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50d9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7189e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Examples:\n",
      "imdb\tNot recommended.  \n",
      "imdb\tAll in all, a great disappointment.  \n",
      "amazon\tFirst of all, it doesn't wear well.\n",
      "yelp\tI do love sushi, but I found Kabuki to be over-priced, over-hip and under-services.\n",
      "yelp\tFor service, I give them no stars.\n",
      "\n",
      "False Nagetive Examples:\n",
      "amazon\tWould recommend this item.\n",
      "imdb\tI struggle to find anything bad to say about it.  \n",
      "imdb\tGive this one a look.  \n",
      "yelp\tFood was so gooodd.\n",
      "yelp\tI could eat their bruschetta all day it is devine.\n"
     ]
    }
   ],
   "source": [
    "FP = list(np.where((yhat_tr_N == 1) & (y_true == 0))[0])\n",
    "FN = list(np.where((yhat_tr_N == 0) & (y_true == 1))[0])\n",
    "\n",
    "# debug you\n",
    "# print(yhat_tr_N)\n",
    "# print(y_true)\n",
    "# print(len(yhat_tr_N), len(y_true))\n",
    "# print(FP)\n",
    "# print(FN)\n",
    "FP_list = list()\n",
    "FN_list = list()\n",
    "\n",
    "rand_5_examples_FP = random.sample(FP, 5)\n",
    "rand_5_examples_FN = random.sample(FN, 5)\n",
    "# print(rand_5_examples_FP)\n",
    "\n",
    "\n",
    "for index in rand_5_examples_FP:\n",
    "    result = x_train_df['website_name'][index], x_train_df['text'][index]\n",
    "    FP_list.append(result)\n",
    "    \n",
    "for index in rand_5_examples_FN:\n",
    "    result = x_train_df['website_name'][index], x_train_df['text'][index]\n",
    "    FN_list.append(result)\n",
    "    \n",
    "print(\"False Positive Examples:\")\n",
    "for row in FP_list:\n",
    "    print('\\t'.join(row))\n",
    "    \n",
    "print()\n",
    "print(\"False Nagetive Examples:\")\n",
    "for row in FN_list:\n",
    "    print('\\t'.join(row))\n",
    "# website_name = x_train_df['website_name']\n",
    "# text = x_train_df['text']\n",
    "\n",
    "# print(\"False Positives:\")\n",
    "# for idx in FP:\n",
    "#     print(f\"Website: {website_name[idx]}\")\n",
    "#     print(f\"Text: {text[idx]}\")\n",
    "#     print(f\"Predicted: {yhat_test_N[idx]}\")\n",
    "#     print(f\"Actual: {y_true[idx]}\")\n",
    "#     print()\n",
    "\n",
    "# print(\"False Negatives:\")\n",
    "# for idx in FN:\n",
    "#     print(f\"Website: {website_name[idx]}\")\n",
    "#     print(f\"Text: {text[idx]}\")\n",
    "#     print(f\"Predicted: {yhat_test_N[idx]}\")\n",
    "#     print(f\"Actual: {y_true[idx]}\")\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d890315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504faa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
