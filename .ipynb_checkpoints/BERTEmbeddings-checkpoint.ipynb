{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings with BERT (Bidirectional Encoder Representations from Transformers) \n",
    "\n",
    "### What is BERT?\n",
    "BERT is a large state-of-the-art neural network that has been trained on a large corpora of text (millions of sentences). Its applications include but are not limited to:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Text classification\n",
    "- Question answering systems\n",
    " \n",
    "In this notebook, we walk through how BERT generates fixed-length embeddings (features) from a sentence. You could think of these embeddings as an alternate feature extraction technique compared to bag of words. The BERT model has 2 main components as shown below \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries (if not already installed)\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer (Converting sentences into series of numerical tokens):\n",
    "\n",
    "The tokenizer in BERT is like a translator that converts sentences into a series of numerical tokens that the BERT model can understand. Specifically, it does the following:\n",
    "\n",
    "- Splits Text: It breaks down sentences into smaller pieces called tokens. These tokens can be as short as one character or as long as one word. For example, the word \"chatting\" might be split into \"chat\" and \"##ting\".\n",
    "\n",
    "- Converts Tokens to IDs: Each token has a unique ID in BERT's vocabulary. The tokenizer maps every token to its corresponding ID. This is like looking up the \"meaning\" of the word in BERT's dictionary.\n",
    "\n",
    "- Adds Special Tokens: BERT requires certain special tokens for its tasks, like [CLS] at the beginning of a sentence and [SEP] at the end or between two sentences. The tokenizer adds these in.\n",
    "\n",
    "\n",
    "### Example usage of the tokenizer\n",
    "\n",
    "In the cell below, we see how BERT tokenizes 3 sentences and decodes them back.\n",
    "\n",
    "We'll use the following example sentences:\n",
    "\n",
    "1. \"The sky is blue.\"\n",
    "2. \"Sky is clear today.\"\n",
    "3. \"Look at the clear blue sky.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # Load pre-trained BERT tokenizer and model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sky is blue.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSky is clear today.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLook at the clear blue sky.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# # Load pre-trained BERT tokenizer and model\n",
    "sentences = [\"The sky is blue.\", \"Sky is clear today.\", \"Look at the clear blue sky.\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_text = tokenizer(sentences, padding=True,\n",
    "                         max_length=10,\n",
    "                         truncation=True)['input_ids']\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Examples of tokenizing the sentences with BERT')\n",
    "print('----------------------------------------------')\n",
    "for jj, txt in enumerate(sentences):\n",
    "    print('%s is enocoded as : %s'%(txt, encoded_text[jj]))\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Examples of decoding the tokens back to English')\n",
    "print('----------------------------------------------')\n",
    "for enc in encoded_text:\n",
    "    decoded_text = tokenizer.decode(enc)\n",
    "    print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model (Extracting meaningful feature representations from the sentences):\n",
    "\n",
    "Once the text is tokenized and converted into the necessary format, it's fed into the BERT model. \n",
    "The **model** processes these inputs to generate contextual embeddings or representations for each token. These representations can then be utilized for various downstream tasks like classification, entity recognition, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# Initialize BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embedding(sentence_list, pooling_strategy='cls'):\n",
    "    embedding_list = []\n",
    "    for nn, sentence in enumerate(sentence_list):\n",
    "        if (nn%100==0)&(nn>0):\n",
    "            print('Done with %d sentences'%nn)\n",
    "        \n",
    "        # Tokenize the sentence and get the output from BERT\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Take the embeddings from the last hidden state (optionally, one can use pooling techniques for different representations)\n",
    "        # Here, we take the [CLS] token representation as the sentence embedding\n",
    "        last_hidden_states = outputs.last_hidden_state[0]\n",
    "        \n",
    "        # Pooling strategies\n",
    "        if pooling_strategy == \"cls\":\n",
    "            sentence_embedding = last_hidden_states[0]\n",
    "        elif pooling_strategy == \"mean\":\n",
    "            sentence_embedding = torch.mean(last_hidden_states, dim=0)\n",
    "        elif pooling_strategy == \"max\":\n",
    "            sentence_embedding, _ = torch.max(last_hidden_states, dim=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {pooling_strategy}\")\n",
    "        \n",
    "        embedding_list.append(sentence_embedding)\n",
    "    return torch.stack(embedding_list)\n",
    "\n",
    "sentence = [sentences[0]]\n",
    "embedding = get_bert_embedding(sentence)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print('The sentence \"%s\" has been converted to a feature representation of shape %s'%(sentence[0], embedding.numpy().shape))\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print(embedding.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings from BERT for the movie review train and test sets\n",
    "\n",
    "Below, we generate the BERT embeddings for the movie reviews dataset provided to you. The embeddings are stored as numpy files in the data_reviews folder:\n",
    "\n",
    "- x_train_BERT_embeddings.npy : matrix of size 2400 x 768 containing 768 length features for each of the 2400 sentences in the training set\n",
    "- x_test_BERT_embeddings.npy : matrix of size 600 x 768 containing 768 length features for each of the 600 sentences in the test set\n",
    "\n",
    "Please note that these embeddings are **already provided to you in the data_reviews folder**. You can directly use the provided feature embeddings as inputs to your choice of classifier. If you would like to generate these feature representations again, run the code cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "x_train_df = pd.read_csv('../data_reviews/x_train.csv')\n",
    "x_test_df = pd.read_csv('../data_reviews/x_test.csv')\n",
    "\n",
    "tr_text_list = x_train_df['text'].values.tolist()\n",
    "te_text_list = x_test_df['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for train sequences...\n",
      "Done with 0 sentences\n",
      "Done with 100 sentences\n",
      "Done with 200 sentences\n",
      "Done with 300 sentences\n",
      "Done with 400 sentences\n",
      "Done with 500 sentences\n",
      "Done with 600 sentences\n",
      "Done with 700 sentences\n",
      "Done with 800 sentences\n",
      "Done with 900 sentences\n",
      "Done with 1000 sentences\n",
      "Done with 1100 sentences\n",
      "Done with 1200 sentences\n",
      "Done with 1300 sentences\n",
      "Done with 1400 sentences\n",
      "Done with 1500 sentences\n",
      "Done with 1600 sentences\n",
      "Done with 1700 sentences\n",
      "Done with 1800 sentences\n",
      "Done with 1900 sentences\n",
      "Done with 2000 sentences\n",
      "Done with 2100 sentences\n",
      "Done with 2200 sentences\n",
      "Done with 2300 sentences\n",
      "Generating embeddings for test sequences...\n",
      "Done with 0 sentences\n",
      "Done with 100 sentences\n",
      "Done with 200 sentences\n",
      "Done with 300 sentences\n",
      "Done with 400 sentences\n",
      "Done with 500 sentences\n"
     ]
    }
   ],
   "source": [
    "print('Generating embeddings for train sequences...')\n",
    "tr_embedding = get_bert_embedding(tr_text_list)\n",
    "\n",
    "print('Generating embeddings for test sequences...')\n",
    "te_embedding = get_bert_embedding(te_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the train and test embeddings to /cluster/tufts/hugheslab/prath01/projects/cs135-23f-staffonly/proj_src/projA/data_reviews\n"
     ]
    }
   ],
   "source": [
    "tr_embeddings_ND = tr_embedding.numpy()\n",
    "te_embeddings_ND = te_embedding.numpy()\n",
    "\n",
    "save_dir = os.path.abspath('../data_reviews/')\n",
    "print('Saving the train and test embeddings to %s'%save_dir)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'x_train_BERT_embeddings.npy'), tr_embeddings_ND)\n",
    "np.save(os.path.join(save_dir, 'x_test_BERT_embeddings.npy'), te_embeddings_ND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show similarity between reviews using the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_k_nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "Worst customer service.\n",
      "------------------------------------------------------------------------------------\n",
      "0) Worst customer service.\n",
      "1) Poor product.\n",
      "2) poor quality and service.\n",
      "3) Bad Quality.\n",
      "4) Bad Reception.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "I'm very disappointed with my decision.\n",
      "------------------------------------------------------------------------------------\n",
      "0) I'm very disappointed with my decision.\n",
      "1) I'm a bit disappointed.\n",
      "2) I was very disappointed in the movie.  \n",
      "3) I'm still trying to get over how bad it was.  \n",
      "4) I can't wait to go back.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "There's a horrible tick sound in the background on all my calls that I have never experienced before.\n",
      "------------------------------------------------------------------------------------\n",
      "0) There's a horrible tick sound in the background on all my calls that I have never experienced before.\n",
      "1) The only thing that I think could improve is the sound leaks out from the headset.\n",
      "2) I'm really disappointed all I have now is a charger that doesn't work.\n",
      "3) This is the first phone I've had that has been so cheaply made.\n",
      "4) I've also had problems with the phone reading the memory card in which I always turn it on and then off again.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "It doesn't make you look cool.\n",
      "------------------------------------------------------------------------------------\n",
      "0) It doesn't make you look cool.\n",
      "1) If you see it, you should probably just leave it on the shelf.  \n",
      "2) Seriously, it's not worth wasting your, or your kid's time on.  \n",
      "3) You'll love how thin it is.\n",
      "4) You can't beat the price on these.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "After my phone got to be about a year old, it's been slowly breaking despite much care on my part.\n",
      "------------------------------------------------------------------------------------\n",
      "0) After my phone got to be about a year old, it's been slowly breaking despite much care on my part.\n",
      "1) It is practically useless and did not add any kind of boost to my reception after I bought it.\n",
      "2) i got this phone around the end of may and i'm completely unhappy with it.\n",
      "3) I've also had problems with the phone reading the memory card in which I always turn it on and then off again.\n",
      "4) I put the latest OS on it (v1.15g), and it now likes to slow to a crawl and lock up every once in a while.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "The only thing that disappoint me is the infra red port (irda).\n",
      "------------------------------------------------------------------------------------\n",
      "0) The only thing that disappoint me is the infra red port (irda).\n",
      "1) The biggest complaint I have is, the battery drains superfast.\n",
      "2) That's a huge design flaw (unless I'm not using it correctly, which I don't think is the case).\n",
      "3) I have yet to run this new battery below two bars and that's three days without charging.\n",
      "4) In conclusion, I will not bother with this movie because a volcano in Los Angeles is nothing but nonsense.  \n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "It quit working after I'd used it for about 18 months, so I just purchased another one because this is the best headset I've ever owned.\n",
      "------------------------------------------------------------------------------------\n",
      "0) It quit working after I'd used it for about 18 months, so I just purchased another one because this is the best headset I've ever owned.\n",
      "1) I ordered this for sony Ericsson W810i but I think it only worked once (thats when I first used it).\n",
      "2) It's been my choice headset for years.Great sound; good volume; good noise cancellation.\n",
      "3) I've bought $5 wired headphones that sound better than these.\n",
      "4) It's uncomfortable and the sound quality is quite poor compared with the phone (Razr) or with my previous wired headset (that plugged into an LG).\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "Unfortunately it did not work.\n",
      "------------------------------------------------------------------------------------\n",
      "0) Unfortunately it did not work.\n",
      "1) It worked very well.\n",
      "2) The charger arrived within the promised timeframe, but it did not work.\n",
      "3) The plug did not work very well.\n",
      "4) THAT one didn't work either.\n",
      "------------------------------------------------------------------------------------\n",
      "Reviews that resemble to the sentence : \n",
      "Excellent bluetooth headset.\n",
      "------------------------------------------------------------------------------------\n",
      "0) Excellent bluetooth headset.\n",
      "1) Excellent starter wireless headset.\n",
      "2) Excellent dual-purpose headset.\n",
      "3) Great Earpiece.\n",
      "4) Excellent wallet type phone case.\n"
     ]
    }
   ],
   "source": [
    "# choose some query sentences\n",
    "sentence_id_list = [5, 20, 70, 85, 92, 12, 521, 100, 712]\n",
    "\n",
    "# use K-nearest neighbors to find the 5 reviews that most closely resemble the query review\n",
    "for sentence_id in sentence_id_list:\n",
    "    query_QF = tr_embeddings_ND[sentence_id][np.newaxis, :]\n",
    "    _, nearest_ids_per_query = calc_k_nearest_neighbors(tr_embeddings_ND, query_QF, K=5)\n",
    "    nearest_ids = nearest_ids_per_query[0]\n",
    "\n",
    "    print('------------------------------------------------------------------------------------')\n",
    "    print('Reviews that resemble to the sentence : \\n%s'%tr_text_list[sentence_id])\n",
    "    print('------------------------------------------------------------------------------------')\n",
    "    for ii, idx in enumerate(nearest_ids):\n",
    "        print('%d) %s'%(ii, tr_text_list[idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
